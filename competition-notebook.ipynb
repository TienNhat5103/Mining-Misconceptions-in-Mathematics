{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7afe70ab",
   "metadata": {
    "papermill": {
     "duration": 0.005941,
     "end_time": "2024-12-12T07:53:00.387586",
     "exception": false,
     "start_time": "2024-12-12T07:53:00.381645",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3749181e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T07:53:00.399072Z",
     "iopub.status.busy": "2024-12-12T07:53:00.398806Z",
     "iopub.status.idle": "2024-12-12T07:53:46.855545Z",
     "shell.execute_reply": "2024-12-12T07:53:46.854401Z"
    },
    "papermill": {
     "duration": 46.465342,
     "end_time": "2024-12-12T07:53:46.857732",
     "exception": false,
     "start_time": "2024-12-12T07:53:00.392390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers peft accelerate \\\n",
    "    -U --no-index --find-links /kaggle/input/lmsys-wheel-files\n",
    "!pip install --no-index /kaggle/input/bitsandbytes0-42-0/bitsandbytes-0.42.0-py3-none-any.whl --find-links=/kaggle/input/bitsandbytes0-42-0\n",
    "#can install triton truoc autoawq\n",
    "!pip install --no-index /kaggle/input/eedi-library-2/triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install --no-index /kaggle/input/eedi-library-2/autoawq-0.2.7.post2-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a6a6ff6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-12T07:53:46.869041Z",
     "iopub.status.busy": "2024-12-12T07:53:46.868749Z",
     "iopub.status.idle": "2024-12-12T07:54:05.311977Z",
     "shell.execute_reply": "2024-12-12T07:54:05.311261Z"
    },
    "papermill": {
     "duration": 18.451028,
     "end_time": "2024-12-12T07:54:05.313950",
     "exception": false,
     "start_time": "2024-12-12T07:53:46.862922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, math, numpy as np\n",
    "import sys\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re, gc\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import peft\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "pd.set_option('display.max_rows', 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacae9eb",
   "metadata": {
    "papermill": {
     "duration": 0.004533,
     "end_time": "2024-12-12T07:54:05.323682",
     "exception": false,
     "start_time": "2024-12-12T07:54:05.319149",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b939f54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T07:54:05.335321Z",
     "iopub.status.busy": "2024-12-12T07:54:05.334465Z",
     "iopub.status.idle": "2024-12-12T07:54:05.339282Z",
     "shell.execute_reply": "2024-12-12T07:54:05.338554Z"
    },
    "papermill": {
     "duration": 0.01265,
     "end_time": "2024-12-12T07:54:05.340860",
     "exception": false,
     "start_time": "2024-12-12T07:54:05.328210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model paths\n",
    "QW25_MODEL = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "LORA_MODEL = '/kaggle/input/2211-lora-14b/transformers/default/1'\n",
    "QW14B_MODEL = \"/kaggle/input/qw14b-awq/transformers/default/1\"\n",
    "\n",
    "# dataset paths\n",
    "TRAIN_SET = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\"\n",
    "TEST_SET = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\"\n",
    "MISCONCEPTION_MAP_SET = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\"\n",
    "\n",
    "# submission flag\n",
    "IS_SUBMISSION = bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))\n",
    "IS_SUBMISSION = True\n",
    "\n",
    "# parameters\n",
    "MAX_LENGTH = 512\n",
    "LOAD_IN_4BIT = False\n",
    "BATCH_SIZE = 16\n",
    "QUERY_MAX_LEN = 320\n",
    "DOC_MAX_LEN = 48"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2d70dc",
   "metadata": {
    "papermill": {
     "duration": 0.004462,
     "end_time": "2024-12-12T07:54:05.350025",
     "exception": false,
     "start_time": "2024-12-12T07:54:05.345563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6569cd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T07:54:05.361582Z",
     "iopub.status.busy": "2024-12-12T07:54:05.361108Z",
     "iopub.status.idle": "2024-12-12T07:54:05.416762Z",
     "shell.execute_reply": "2024-12-12T07:54:05.416115Z"
    },
    "papermill": {
     "duration": 0.063442,
     "end_time": "2024-12-12T07:54:05.418438",
     "exception": false,
     "start_time": "2024-12-12T07:54:05.354996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAIN_SET).fillna(-1).sample(10, random_state = 42).reset_index(drop = True)\n",
    "df_test = pd.read_csv(TEST_SET)\n",
    "df_misconception_mapping = pd.read_csv(MISCONCEPTION_MAP_SET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79bac43",
   "metadata": {
    "papermill": {
     "duration": 0.004458,
     "end_time": "2024-12-12T07:54:05.427601",
     "exception": false,
     "start_time": "2024-12-12T07:54:05.423143",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Check if Notebook is Submitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "946c70c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T07:54:05.437816Z",
     "iopub.status.busy": "2024-12-12T07:54:05.437566Z",
     "iopub.status.idle": "2024-12-12T07:54:05.441490Z",
     "shell.execute_reply": "2024-12-12T07:54:05.440694Z"
    },
    "papermill": {
     "duration": 0.010666,
     "end_time": "2024-12-12T07:54:05.442928",
     "exception": false,
     "start_time": "2024-12-12T07:54:05.432262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not IS_SUBMISSION:\n",
    "    df_ret = df_train.copy()\n",
    "else:\n",
    "    df_ret = df_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c89324c",
   "metadata": {
    "papermill": {
     "duration": 0.004402,
     "end_time": "2024-12-12T07:54:05.451954",
     "exception": false,
     "start_time": "2024-12-12T07:54:05.447552",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f79a762",
   "metadata": {
    "papermill": {
     "duration": 0.004416,
     "end_time": "2024-12-12T07:54:05.460965",
     "exception": false,
     "start_time": "2024-12-12T07:54:05.456549",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Format Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9746fe17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T07:54:05.471266Z",
     "iopub.status.busy": "2024-12-12T07:54:05.471014Z",
     "iopub.status.idle": "2024-12-12T07:54:05.481737Z",
     "shell.execute_reply": "2024-12-12T07:54:05.480947Z"
    },
    "papermill": {
     "duration": 0.017382,
     "end_time": "2024-12-12T07:54:05.483143",
     "exception": false,
     "start_time": "2024-12-12T07:54:05.465761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEMPLATE_INPUT_V3 = '{QUESTION}\\nCorrect answer: {CORRECT_ANSWER}\\nStudent wrong answer: {STUDENT_WRONG_ANSWER}'\n",
    "def format_input_v3(row, wrong_choice):\n",
    "    assert wrong_choice in \"ABCD\"\n",
    "    question_text = row[\"QuestionText\"]\n",
    "    subject_name = row[\"SubjectName\"]\n",
    "    construct_name = row[\"ConstructName\"]\n",
    "    correct_answer = row[\"CorrectAnswer\"]\n",
    "    assert wrong_choice != correct_answer\n",
    "    correct_answer_text = row[f\"Answer{correct_answer}Text\"]\n",
    "    wrong_answer_text = row[f\"Answer{wrong_choice}Text\"]\n",
    "    formatted_question = f\"Question: {question_text}\\nSubjectName: {subject_name}\\nConstructName: {construct_name}\"\n",
    "\n",
    "    return {\n",
    "        \"QUESTION\": formatted_question,\n",
    "        \"CORRECT_ANSWER\": correct_answer_text,\n",
    "        \"STUDENT_WRONG_ANSWER\": wrong_answer_text,\n",
    "        \"MISCONCEPTION_ID\": row.get(f'Misconception{wrong_choice}Id'),\n",
    "        \"PROMPT\": TEMPLATE_INPUT_V3.format(\n",
    "            QUESTION = formatted_question, \n",
    "            CORRECT_ANSWER = correct_answer_text, \n",
    "            STUDENT_WRONG_ANSWER = wrong_answer_text\n",
    "        )\n",
    "    }\n",
    "items = []\n",
    "target_ids = []\n",
    "for _, row in df_ret.iterrows():\n",
    "    for choice in \"ABCD\":\n",
    "        if choice == row[\"CorrectAnswer\"]:\n",
    "            continue\n",
    "        if not IS_SUBMISSION and row[f'Misconception{choice}Id'] == -1:\n",
    "            continue\n",
    "        item = {\n",
    "            \"QuestionId_Answer\": f'{row[\"QuestionId\"]}_{choice}',  \n",
    "            \"Prompt\": format_input_v3(row, choice)[\"PROMPT\"]  \n",
    "        }\n",
    "        items.append(item)\n",
    "        misconception_id = row.get(f'Misconception{choice}Id', -1)\n",
    "        target_ids.append(int(misconception_id))\n",
    "\n",
    "df_input = pd.DataFrame(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bcd2ae",
   "metadata": {
    "papermill": {
     "duration": 0.004323,
     "end_time": "2024-12-12T07:54:05.491978",
     "exception": false,
     "start_time": "2024-12-12T07:54:05.487655",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "808bddfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T07:54:05.502139Z",
     "iopub.status.busy": "2024-12-12T07:54:05.501892Z",
     "iopub.status.idle": "2024-12-12T07:54:06.120613Z",
     "shell.execute_reply": "2024-12-12T07:54:06.119901Z"
    },
    "papermill": {
     "duration": 0.626546,
     "end_time": "2024-12-12T07:54:06.123135",
     "exception": false,
     "start_time": "2024-12-12T07:54:05.496589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'<instruct>{task_description}\\n<query>{query}'\n",
    "\n",
    "def get_detailed_example(task_description: str, query: str, response: str) -> str:\n",
    "    return f'<instruct>{task_description}\\n<query>{query}\\n<response>{response}'\n",
    "\n",
    "def get_new_queries(queries, query_max_len, examples_prefix, tokenizer):\n",
    "    inputs = tokenizer(\n",
    "        queries,\n",
    "        max_length = query_max_len - len(tokenizer('<s>', add_special_tokens = False)['input_ids']) \n",
    "            - len(tokenizer('\\n<response></s>', add_special_tokens = False)['input_ids']),\n",
    "        return_token_type_ids = False,\n",
    "        truncation = True,\n",
    "        return_tensors = None,\n",
    "        add_special_tokens = False\n",
    "    )\n",
    "    prefix_ids = tokenizer(examples_prefix, add_special_tokens = False)['input_ids']\n",
    "    suffix_ids = tokenizer('\\n<response>', add_special_tokens = False)['input_ids']\n",
    "    new_max_length = (len(prefix_ids) + len(suffix_ids) + query_max_len + 8) // 8 * 8 + 8\n",
    "    new_queries = tokenizer.batch_decode(inputs['input_ids'])\n",
    "    \n",
    "    for i in range(len(new_queries)):\n",
    "        new_queries[i] = examples_prefix + new_queries[i] + '\\n<response>'\n",
    "        \n",
    "    return new_max_length, new_queries\n",
    "\n",
    "task =  \"Given a math problem with correct and incorrect answers, identify the misconception behind the student's incorrect choice.\"\n",
    "queries = [\n",
    "    get_detailed_instruct(task, q) for q in df_input['Prompt']\n",
    "]\n",
    "documents = df_misconception_mapping['MisconceptionName'].tolist()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LORA_MODEL)\n",
    "examples_prefix = ''\n",
    "new_query_max_len, new_queries = get_new_queries(queries, QUERY_MAX_LEN, examples_prefix, tokenizer)\n",
    "\n",
    "data = {'texts': new_queries + documents}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1becd5",
   "metadata": {
    "papermill": {
     "duration": 0.005888,
     "end_time": "2024-12-12T07:54:06.134308",
     "exception": false,
     "start_time": "2024-12-12T07:54:06.128420",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "660e603a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T07:54:06.145441Z",
     "iopub.status.busy": "2024-12-12T07:54:06.145123Z",
     "iopub.status.idle": "2024-12-12T07:54:06.153507Z",
     "shell.execute_reply": "2024-12-12T07:54:06.152812Z"
    },
    "papermill": {
     "duration": 0.015559,
     "end_time": "2024-12-12T07:54:06.155092",
     "exception": false,
     "start_time": "2024-12-12T07:54:06.139533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "folds = [[0, 5], [1, 5], [2, 5], [3, 5], [4, 5]]\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = attention_mask[:, -1].sum() == attention_mask.shape[0]\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim = 1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        \n",
    "        return last_hidden_states[\n",
    "            torch.arange(batch_size, device = last_hidden_states.device), sequence_lengths\n",
    "        ]\n",
    "\n",
    "def get_embeddings_in_batches(model, tokenizer, texts, max_length, batch_size = 16):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc = \"Embedding\"):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "        batch_dict = tokenizer(\n",
    "            batch_texts,\n",
    "            max_length = max_length,\n",
    "            padding = True,\n",
    "            truncation = True,\n",
    "            return_tensors = \"pt\",\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad(), torch.amp.autocast(\"cuda\"):\n",
    "            outputs = model(**batch_dict)\n",
    "            batch_embeddings = last_token_pool(\n",
    "                outputs.last_hidden_state, batch_dict[\"attention_mask\"]\n",
    "            )\n",
    "            batch_embeddings = F.normalize(batch_embeddings, p = 2, dim = 1).cpu()\n",
    "        embeddings.append(batch_embeddings)\n",
    "        \n",
    "    return torch.cat(embeddings, dim = 0)\n",
    "\n",
    "def load_model_and_tokenizer(base_model_path, lora_path, load_in_4bit = False):\n",
    "    model = AutoModel.from_pretrained(\n",
    "        base_model_path,\n",
    "        device_map = 0,\n",
    "        torch_dtype = torch.float16,\n",
    "        load_in_4bit= load_in_4bit,  \n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        lora_path if lora_path else base_model_path\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    if lora_path:\n",
    "        model = peft.PeftModel.from_pretrained(model, lora_path)\n",
    "        \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600a6f8d",
   "metadata": {
    "papermill": {
     "duration": 0.004629,
     "end_time": "2024-12-12T07:54:06.164402",
     "exception": false,
     "start_time": "2024-12-12T07:54:06.159773",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Building and Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e1a76c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T07:54:06.174472Z",
     "iopub.status.busy": "2024-12-12T07:54:06.174206Z",
     "iopub.status.idle": "2024-12-12T07:55:27.510259Z",
     "shell.execute_reply": "2024-12-12T07:55:27.509540Z"
    },
    "papermill": {
     "duration": 81.343235,
     "end_time": "2024-12-12T07:55:27.512180",
     "exception": false,
     "start_time": "2024-12-12T07:54:06.168945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27eb22c51d694151829962d98ac0af0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = load_model_and_tokenizer(\n",
    "    QW14B_MODEL, LORA_MODEL, load_in_4bit = LOAD_IN_4BIT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ab0037f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T07:55:27.523663Z",
     "iopub.status.busy": "2024-12-12T07:55:27.523277Z",
     "iopub.status.idle": "2024-12-12T08:01:48.407703Z",
     "shell.execute_reply": "2024-12-12T08:01:48.406842Z"
    },
    "papermill": {
     "duration": 380.892289,
     "end_time": "2024-12-12T08:01:48.409555",
     "exception": false,
     "start_time": "2024-12-12T07:55:27.517266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 1/1 [00:05<00:00,  5.12s/it]\n",
      "Embedding: 100%|██████████| 162/162 [06:15<00:00,  2.32s/it]\n"
     ]
    }
   ],
   "source": [
    "V_queries = get_embeddings_in_batches(model, tokenizer, new_queries, max_length = QUERY_MAX_LEN, batch_size = BATCH_SIZE)\n",
    "V_documents = get_embeddings_in_batches(model, tokenizer, documents, max_length = DOC_MAX_LEN, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372d0b7d",
   "metadata": {
    "papermill": {
     "duration": 0.011642,
     "end_time": "2024-12-12T08:01:48.437830",
     "exception": false,
     "start_time": "2024-12-12T08:01:48.426188",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Negative Mining (maybe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f82b5f95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T08:01:48.463424Z",
     "iopub.status.busy": "2024-12-12T08:01:48.463054Z",
     "iopub.status.idle": "2024-12-12T08:01:49.179597Z",
     "shell.execute_reply": "2024-12-12T08:01:49.178385Z"
    },
    "papermill": {
     "duration": 0.733512,
     "end_time": "2024-12-12T08:01:49.183215",
     "exception": false,
     "start_time": "2024-12-12T08:01:48.449703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# using k-NN to find top-k nearest objects (indices)\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "neighbors_model = NearestNeighbors(n_neighbors = 25, \n",
    "                                   metric = 'cosine', \n",
    "                                   algorithm = \"brute\", \n",
    "                                   n_jobs = -1)\n",
    "neighbors_model.fit(V_documents)\n",
    "dists, indices = neighbors_model.kneighbors(V_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6f154ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T08:01:49.263226Z",
     "iopub.status.busy": "2024-12-12T08:01:49.260425Z",
     "iopub.status.idle": "2024-12-12T08:01:49.300778Z",
     "shell.execute_reply": "2024-12-12T08:01:49.299614Z"
    },
    "papermill": {
     "duration": 0.082179,
     "end_time": "2024-12-12T08:01:49.303699",
     "exception": false,
     "start_time": "2024-12-12T08:01:49.221520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_text</th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>ConstructName</th>\n",
       "      <th>SubjectName</th>\n",
       "      <th>QuestionText</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>incorrect_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>### SubjectName: BIDMAS\\n### ConstructName: Us...</td>\n",
       "      <td>1869_B</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>\\( 3 \\times 2+(4-5) \\)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>### SubjectName: BIDMAS\\n### ConstructName: Us...</td>\n",
       "      <td>1869_C</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>\\( 3 \\times(2+4-5) \\)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>### SubjectName: BIDMAS\\n### ConstructName: Us...</td>\n",
       "      <td>1869_D</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>Does not need brackets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>### SubjectName: Simplifying Algebraic Fractio...</td>\n",
       "      <td>1870_A</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>\\( m+1 \\)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>### SubjectName: Simplifying Algebraic Fractio...</td>\n",
       "      <td>1870_B</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>\\( m+2 \\)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          query_text QuestionId_Answer  \\\n",
       "0  ### SubjectName: BIDMAS\\n### ConstructName: Us...            1869_B   \n",
       "1  ### SubjectName: BIDMAS\\n### ConstructName: Us...            1869_C   \n",
       "2  ### SubjectName: BIDMAS\\n### ConstructName: Us...            1869_D   \n",
       "3  ### SubjectName: Simplifying Algebraic Fractio...            1870_A   \n",
       "4  ### SubjectName: Simplifying Algebraic Fractio...            1870_B   \n",
       "\n",
       "                                       ConstructName  \\\n",
       "0  Use the order of operations to carry out calcu...   \n",
       "1  Use the order of operations to carry out calcu...   \n",
       "2  Use the order of operations to carry out calcu...   \n",
       "3  Simplify an algebraic fraction by factorising ...   \n",
       "4  Simplify an algebraic fraction by factorising ...   \n",
       "\n",
       "                       SubjectName  \\\n",
       "0                           BIDMAS   \n",
       "1                           BIDMAS   \n",
       "2                           BIDMAS   \n",
       "3  Simplifying Algebraic Fractions   \n",
       "4  Simplifying Algebraic Fractions   \n",
       "\n",
       "                                        QuestionText         correct_answer  \\\n",
       "0  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "1  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "2  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "3  Simplify the following, if possible: \\( \\frac{...      Does not simplify   \n",
       "4  Simplify the following, if possible: \\( \\frac{...      Does not simplify   \n",
       "\n",
       "         incorrect_answer  \n",
       "0  \\( 3 \\times 2+(4-5) \\)  \n",
       "1   \\( 3 \\times(2+4-5) \\)  \n",
       "2  Does not need brackets  \n",
       "3               \\( m+1 \\)  \n",
       "4               \\( m+2 \\)  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "for idx, row in df_ret.iterrows():\n",
    "    for option in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "        if option == row.CorrectAnswer:\n",
    "            continue\n",
    "            \n",
    "        correct_answer = row[f\"Answer{row.CorrectAnswer}Text\"]\n",
    "\n",
    "        query_text =f\"### SubjectName: {row['SubjectName']}\\n### ConstructName: {row['ConstructName']}\\n### Question: {row['QuestionText']}\\n### Correct Answer: {correct_answer}\\n### Misconcepte Incorrect answer: {option}.{row[f'Answer{option}Text']}\"\n",
    "        rows.append({\"query_text\": query_text, \n",
    "                     \"QuestionId_Answer\": f\"{row.QuestionId}_{option}\",\n",
    "                     \"ConstructName\": row.ConstructName,\n",
    "                     \"SubjectName\": row.SubjectName,\n",
    "                     \"QuestionText\": row.QuestionText,\n",
    "                     \"correct_answer\": correct_answer,\n",
    "                     \"incorrect_answer\": row[f\"Answer{option}Text\"]\n",
    "                     })\n",
    "\n",
    "summarized_df = pd.DataFrame(rows)\n",
    "summarized_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65981524",
   "metadata": {
    "papermill": {
     "duration": 0.011918,
     "end_time": "2024-12-12T08:01:49.327669",
     "exception": false,
     "start_time": "2024-12-12T08:01:49.315751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Free Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07013454",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T08:01:49.353104Z",
     "iopub.status.busy": "2024-12-12T08:01:49.352838Z",
     "iopub.status.idle": "2024-12-12T08:01:49.939932Z",
     "shell.execute_reply": "2024-12-12T08:01:49.938970Z"
    },
    "papermill": {
     "duration": 0.601912,
     "end_time": "2024-12-12T08:01:49.942040",
     "exception": false,
     "start_time": "2024-12-12T08:01:49.340128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if \"model\" in globals():\n",
    "    del model\n",
    "if \"tokenizer\" in globals():\n",
    "    del tokenizer\n",
    "if \"V_queries\" in globals():\n",
    "    del V_queries\n",
    "if \"V_documents\" in globals():\n",
    "    del V_documents\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d52b24",
   "metadata": {
    "papermill": {
     "duration": 0.01208,
     "end_time": "2024-12-12T08:01:49.967526",
     "exception": false,
     "start_time": "2024-12-12T08:01:49.955446",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Save Data for Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4a80b63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T08:01:49.993111Z",
     "iopub.status.busy": "2024-12-12T08:01:49.992811Z",
     "iopub.status.idle": "2024-12-12T08:01:50.033271Z",
     "shell.execute_reply": "2024-12-12T08:01:50.032652Z"
    },
    "papermill": {
     "duration": 0.055088,
     "end_time": "2024-12-12T08:01:50.034957",
     "exception": false,
     "start_time": "2024-12-12T08:01:49.979869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save(\"indices.npy\", indices)\n",
    "summarized_df.to_parquet(\"df.parquet\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d239f1b0",
   "metadata": {
    "papermill": {
     "duration": 0.011655,
     "end_time": "2024-12-12T08:01:50.059393",
     "exception": false,
     "start_time": "2024-12-12T08:01:50.047738",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Apply Logits Processor Zoo for VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9fdd498",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T08:01:50.084364Z",
     "iopub.status.busy": "2024-12-12T08:01:50.084060Z",
     "iopub.status.idle": "2024-12-12T08:05:24.929757Z",
     "shell.execute_reply": "2024-12-12T08:05:24.928602Z"
    },
    "papermill": {
     "duration": 214.860429,
     "end_time": "2024-12-12T08:05:24.931635",
     "exception": false,
     "start_time": "2024-12-12T08:01:50.071206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.4.0\r\n",
      "Uninstalling torch-2.4.0:\r\n",
      "  Successfully uninstalled torch-2.4.0\r\n",
      "Processing /kaggle/input/logits-processor-zoo/logits_processor_zoo-0.1.0-py3-none-any.whl\r\n",
      "Installing collected packages: logits-processor-zoo\r\n",
      "Successfully installed logits-processor-zoo-0.1.0\r\n",
      "CPU times: user 2.36 s, sys: 796 ms, total: 3.16 s\n",
      "Wall time: 3min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!pip uninstall -y torch\n",
    "!pip install -q --no-index --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-vllm vllm\n",
    "!pip install -q -U /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install -q -U /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl\n",
    "!pip install -q --no-deps --no-index /kaggle/input/hf-libraries/sentence-transformers/sentence_transformers-3.1.0-py3-none-any.whl\n",
    "!pip install --no-deps --no-index /kaggle/input/logits-processor-zoo/logits_processor_zoo-0.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f5a753b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T08:05:24.958831Z",
     "iopub.status.busy": "2024-12-12T08:05:24.958548Z",
     "iopub.status.idle": "2024-12-12T08:05:24.967176Z",
     "shell.execute_reply": "2024-12-12T08:05:24.966262Z"
    },
    "papermill": {
     "duration": 0.024346,
     "end_time": "2024-12-12T08:05:24.968773",
     "exception": false,
     "start_time": "2024-12-12T08:05:24.944427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_vllm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_vllm.py\n",
    "\n",
    "import vllm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizer, AutoTokenizer\n",
    "from typing import List\n",
    "import torch\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "import re\n",
    "\n",
    "MISCONCEPTION_MAP_SET = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\"\n",
    "QW25_MODEL = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(QW25_MODEL)\n",
    "\n",
    "def preprocess_text(x):\n",
    "    x = re.sub(\"http\\w+\", '',x)   # Delete URL\n",
    "    x = re.sub(r\"\\.+\", \".\", x)    # Replace consecutive commas and periods with one comma and period character\n",
    "    x = re.sub(r\"\\,+\", \",\", x)\n",
    "    x = re.sub(r\"\\\\\\(\", \" \", x)\n",
    "    x = re.sub(r\"\\\\\\)\", \" \", x)\n",
    "    x = re.sub(r\"[ ]{1,}\", \" \", x)\n",
    "    x = x.strip()                 # Remove empty characters at the beginning and end\n",
    "    return x\n",
    "\n",
    "PROMPT = \"\"\"Here is a question about {ConstructName}({SubjectName}).\n",
    "Question: {Question}\n",
    "Correct Answer: {CorrectAnswer}\n",
    "Incorrect Answer: {IncorrectAnswer}\n",
    "\n",
    "You are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\n",
    "Answer concisely what misconception it is to lead to getting the incorrect answer.\n",
    "Pick the correct misconception number from the below:\n",
    "\n",
    "{Retrival}\n",
    "\"\"\"\n",
    "\n",
    "def apply_template(row, tokenizer):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": preprocess_text(\n",
    "                PROMPT.format(\n",
    "                    ConstructName = row[\"ConstructName\"],\n",
    "                    SubjectName = row[\"SubjectName\"],\n",
    "                    Question = row[\"QuestionText\"],\n",
    "                    IncorrectAnswer = row[f\"incorrect_answer\"],\n",
    "                    CorrectAnswer = row[f\"correct_answer\"],\n",
    "                    Retrival = row[f\"retrieval\"]\n",
    "                )\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\n",
    "    return text\n",
    "\n",
    "df_misconception_map = pd.read_csv(MISCONCEPTION_MAP_SET)\n",
    "\n",
    "summarized_df = pd.read_parquet(\"df.parquet\")\n",
    "indices = np.load(\"indices.npy\")\n",
    "\n",
    "def get_candidates(c_indices):\n",
    "    candidates = []\n",
    "\n",
    "    mis_names = df_misconception_map[\"MisconceptionName\"].values\n",
    "    for ix in c_indices:\n",
    "        c_names = []\n",
    "        for i, name in enumerate(mis_names[ix]):\n",
    "            c_names.append(f\"{i+1}. {name}\")\n",
    "\n",
    "        candidates.append(\"\\n\".join(c_names))\n",
    "        \n",
    "    return candidates\n",
    "\n",
    "llm = vllm.LLM(\n",
    "    QW25_MODEL,\n",
    "    quantization = \"awq\",\n",
    "    tensor_parallel_size = 2,\n",
    "    gpu_memory_utilization = 0.90, \n",
    "    trust_remote_code = True,\n",
    "    dtype = \"half\", \n",
    "    enforce_eager = True,\n",
    "    max_model_len = 5120,\n",
    "    disable_log_stats = True\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "survivors = indices[:, -1:]\n",
    "\n",
    "for i in range(3):\n",
    "    c_indices = np.concatenate([indices[:, -8*(i+1)-1:-8*i-1], survivors], axis = 1)\n",
    "    \n",
    "    summarized_df[\"retrieval\"] = get_candidates(c_indices)\n",
    "    summarized_df[\"text\"] = summarized_df.apply(lambda row: apply_template(row, tokenizer), axis = 1)\n",
    "    \n",
    "    print(\"Example:\")\n",
    "    print(summarized_df[\"text\"].values[0])\n",
    "    print()\n",
    "    \n",
    "    responses = llm.generate(\n",
    "        summarized_df[\"text\"].values,\n",
    "        vllm.SamplingParams(\n",
    "            n = 1,  # Number of output sequences to return for each prompt.\n",
    "            top_k = 1,  # Float that controls the cumulative probability of the top tokens to consider.\n",
    "            temperature = 0,  # randomness of the sampling\n",
    "            seed = 777, # Seed for reprodicibility\n",
    "            skip_special_tokens = False,  # Whether to skip special tokens in the output.\n",
    "            max_tokens = 1,  # Maximum number of tokens to generate per output sequence.\n",
    "            logits_processors = [MultipleChoiceLogitsProcessor(tokenizer, \n",
    "                                                               choices = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"])]\n",
    "        ),\n",
    "        use_tqdm = True\n",
    "    )\n",
    "    \n",
    "    responses = [x.outputs[0].text for x in responses]\n",
    "    summarized_df[\"response\"] = responses\n",
    "    \n",
    "    llm_choices = summarized_df[\"response\"].astype(int).values - 1\n",
    "    \n",
    "    survivors = np.array([cix[best] for best, cix in zip(llm_choices, c_indices)]).reshape(-1, 1)\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(indices.shape[0]):\n",
    "    ix = indices[i]\n",
    "    llm_choice = survivors[i, 0]\n",
    "    \n",
    "    results.append(\" \".join([str(llm_choice)] + [str(x) for x in ix if x != llm_choice]))\n",
    "\n",
    "\n",
    "summarized_df[\"MisconceptionId\"] = results\n",
    "summarized_df.to_csv(\"submission.csv\", columns = [\"QuestionId_Answer\", \"MisconceptionId\"], index = False)\n",
    "summarized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbae54b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T08:05:24.994894Z",
     "iopub.status.busy": "2024-12-12T08:05:24.994185Z",
     "iopub.status.idle": "2024-12-12T08:07:46.905565Z",
     "shell.execute_reply": "2024-12-12T08:07:46.904365Z"
    },
    "papermill": {
     "duration": 141.928065,
     "end_time": "2024-12-12T08:07:46.909168",
     "exception": false,
     "start_time": "2024-12-12T08:05:24.981103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-12 08:05:30 config.py:246] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "INFO 12-12 08:05:30 config.py:715] Defaulting to use mp for distributed inference\r\n",
      "INFO 12-12 08:05:30 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=5120, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1, use_v2_block_manager=False, enable_prefix_caching=False)\r\n",
      "INFO 12-12 08:05:30 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n",
      "INFO 12-12 08:05:31 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=163)\u001b[0;0m INFO 12-12 08:05:31 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-12 08:05:31 selector.py:54] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=163)\u001b[0;0m INFO 12-12 08:05:31 selector.py:54] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=163)\u001b[0;0m INFO 12-12 08:05:32 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n",
      "INFO 12-12 08:05:33 utils.py:784] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=163)\u001b[0;0m INFO 12-12 08:05:33 utils.py:784] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=163)\u001b[0;0m INFO 12-12 08:05:33 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-12 08:05:33 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-12 08:05:33 custom_all_reduce_utils.py:202] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 12-12 08:05:41 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=163)\u001b[0;0m INFO 12-12 08:05:41 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 12-12 08:05:41 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x789e64557610>, local_subscribe_port=45249, local_sync_port=35263, remote_subscribe_port=None, remote_sync_port=None)\r\n",
      "INFO 12-12 08:05:41 model_runner.py:680] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=163)\u001b[0;0m INFO 12-12 08:05:41 model_runner.py:680] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=163)\u001b[0;0m INFO 12-12 08:05:41 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=163)\u001b[0;0m INFO 12-12 08:05:41 selector.py:54] Using XFormers backend.\r\n",
      "INFO 12-12 08:05:41 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-12 08:05:41 selector.py:54] Using XFormers backend.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:17<01:10, 17.68s/it]\r\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:35<00:52, 17.49s/it]\r\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:53<00:36, 18.05s/it]\r\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [01:13<00:18, 18.74s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [01:32<00:00, 18.64s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [01:32<00:00, 18.40s/it]\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=163)\u001b[0;0m INFO 12-12 08:07:13 model_runner.py:692] Loading model weights took 9.0934 GB\r\n",
      "INFO 12-12 08:07:13 model_runner.py:692] Loading model weights took 9.0934 GB\r\n",
      "INFO 12-12 08:07:22 distributed_gpu_executor.py:56] # GPU blocks: 795, # CPU blocks: 2048\r\n",
      "Example:\r\n",
      "<|im_start|>system\r\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Here is a question about Use the order of operations to carry out calculations involving powers(BIDMAS).\r\n",
      "Question: \\[\r\n",
      "3 \\times 2+4-5\r\n",
      "\\]\r\n",
      "Where do the brackets need to go to make the answer equal 13 ?\r\n",
      "Correct Answer: 3 \\times(2+4)-5 \r\n",
      "Incorrect Answer: 3 \\times 2+(4-5) \r\n",
      "\r\n",
      "You are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\r\n",
      "Answer concisely what misconception it is to lead to getting the incorrect answer.\r\n",
      "Pick the correct misconception number from the below:\r\n",
      "\r\n",
      "1. Does not interpret the correct order of operations from a worded problem\r\n",
      "2. Done a different calculation to the one given\r\n",
      "3. Believes that the order of a worded calculation should be changed to follow BIDMAS \r\n",
      "4. Confuses the order of operations, believes addition comes before division\r\n",
      "5. Performs addition ahead of division\r\n",
      "6. Performs subtraction in wrong order\r\n",
      "7. Has not realised that the answer may be changed by the insertion of brackets\r\n",
      "8. Has completed only one of the two operations.\r\n",
      "9. Does not follow the arrows through a function machine, changes the order of the operations asked.<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "\r\n",
      "\r\n",
      "Processed prompts: 100%|█| 9/9 [00:05<00:00,  1.52it/s, est. speed input: 490.70\r\n",
      "Example:\r\n",
      "<|im_start|>system\r\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Here is a question about Use the order of operations to carry out calculations involving powers(BIDMAS).\r\n",
      "Question: \\[\r\n",
      "3 \\times 2+4-5\r\n",
      "\\]\r\n",
      "Where do the brackets need to go to make the answer equal 13 ?\r\n",
      "Correct Answer: 3 \\times(2+4)-5 \r\n",
      "Incorrect Answer: 3 \\times 2+(4-5) \r\n",
      "\r\n",
      "You are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\r\n",
      "Answer concisely what misconception it is to lead to getting the incorrect answer.\r\n",
      "Pick the correct misconception number from the below:\r\n",
      "\r\n",
      "1. Performs subtraction right to left if priority order means doing a calculation to the right first\r\n",
      "2. Performs addition ahead of any other operation\r\n",
      "3. Answers order of operations questions with brackets as if the brackets are not there\r\n",
      "4. Performs addition ahead of subtraction\r\n",
      "5. Believes order of operations does not affect the answer to a calculation\r\n",
      "6. Subtracts instead of adds\r\n",
      "7. Believes addition comes before indices, in orders of operation\r\n",
      "8. Confuses the order of operations, believes subtraction comes before multiplication \r\n",
      "9. Has not realised that the answer may be changed by the insertion of brackets<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "\r\n",
      "\r\n",
      "Processed prompts: 100%|█| 9/9 [00:05<00:00,  1.70it/s, est. speed input: 545.64\r\n",
      "Example:\r\n",
      "<|im_start|>system\r\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Here is a question about Use the order of operations to carry out calculations involving powers(BIDMAS).\r\n",
      "Question: \\[\r\n",
      "3 \\times 2+4-5\r\n",
      "\\]\r\n",
      "Where do the brackets need to go to make the answer equal 13 ?\r\n",
      "Correct Answer: 3 \\times(2+4)-5 \r\n",
      "Incorrect Answer: 3 \\times 2+(4-5) \r\n",
      "\r\n",
      "You are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\r\n",
      "Answer concisely what misconception it is to lead to getting the incorrect answer.\r\n",
      "Pick the correct misconception number from the below:\r\n",
      "\r\n",
      "1. Carries out operations from right to left regardless of priority order\r\n",
      "2. Carries out operations from left to right regardless of priority order\r\n",
      "3. Inserts brackets but not changed order of operation\r\n",
      "4. Applies BIDMAS in strict order (does not realize addition and subtraction, and multiplication and division, are of equal priority)\r\n",
      "5. Performs addition ahead of multiplication\r\n",
      "6. Confuses the order of operations, believes addition comes before multiplication \r\n",
      "7. Carries out operations from left to right regardless of priority order, unless brackets are used\r\n",
      "8. May have made a calculation error using the order of operations\r\n",
      "9. Confuses the order of operations, believes subtraction comes before multiplication<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "\r\n",
      "\r\n",
      "Processed prompts: 100%|█| 9/9 [00:05<00:00,  1.62it/s, est. speed input: 541.52\r\n",
      "INFO 12-12 08:07:44 multiproc_worker_utils.py:123] Killing local vLLM worker processes\r\n",
      "[rank0]:[W CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\r\n"
     ]
    }
   ],
   "source": [
    "!python run_vllm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4070a0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T08:07:46.937865Z",
     "iopub.status.busy": "2024-12-12T08:07:46.937580Z",
     "iopub.status.idle": "2024-12-12T08:07:46.951026Z",
     "shell.execute_reply": "2024-12-12T08:07:46.950159Z"
    },
    "papermill": {
     "duration": 0.02959,
     "end_time": "2024-12-12T08:07:46.952631",
     "exception": false,
     "start_time": "2024-12-12T08:07:46.923041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>MisconceptionId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>1345 706 1507 2306 328 1672 1005 2518 1963 151...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>1345 2306 1507 706 1005 2488 1999 2532 2518 32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>315 1005 1507 2532 328 1672 1516 2488 706 1345...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>891 2142 2068 418 167 1755 1421 1535 320 2143 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>891 2142 2068 167 1871 1755 341 418 979 2549 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>1755 2142 2068 167 418 891 113 2078 1535 1871 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>1287 1073 1665 2439 1059 1306 1098 1677 2551 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>1287 1073 2439 1059 912 1098 1665 2551 1677 67...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>1287 1073 903 1866 557 1059 912 2471 1700 1975...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer                                    MisconceptionId\n",
       "0            1869_B  1345 706 1507 2306 328 1672 1005 2518 1963 151...\n",
       "1            1869_C  1345 2306 1507 706 1005 2488 1999 2532 2518 32...\n",
       "2            1869_D  315 1005 1507 2532 328 1672 1516 2488 706 1345...\n",
       "3            1870_A  891 2142 2068 418 167 1755 1421 1535 320 2143 ...\n",
       "4            1870_B  891 2142 2068 167 1871 1755 341 418 979 2549 2...\n",
       "5            1870_C  1755 2142 2068 167 418 891 113 2078 1535 1871 ...\n",
       "6            1871_A  1287 1073 1665 2439 1059 1306 1098 1677 2551 6...\n",
       "7            1871_C  1287 1073 2439 1059 912 1098 1665 2551 1677 67...\n",
       "8            1871_D  1287 1073 903 1866 557 1059 912 2471 1700 1975..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9738540,
     "sourceId": 82695,
     "sourceType": "competition"
    },
    {
     "datasetId": 4871830,
     "sourceId": 8218776,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5297895,
     "sourceId": 8897601,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5251603,
     "sourceId": 9094368,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6117312,
     "sourceId": 9948011,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4581967,
     "sourceId": 10171817,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 200567623,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 210735459,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 123481,
     "modelInstanceId": 99392,
     "sourceId": 118192,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 171421,
     "modelInstanceId": 148911,
     "sourceId": 174909,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 171434,
     "modelInstanceId": 148923,
     "sourceId": 174921,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 891.957205,
   "end_time": "2024-12-12T08:07:50.014563",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-12T07:52:58.057358",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0428287b02164e28bdf38fcfad11f2df": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0f3c7228871447a9b38f12cefa447b17": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "172c0566f091427585ea993d10b3964c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "26eb68657e7d4352aff1256dace691b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "27eb22c51d694151829962d98ac0af0c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_cd86ccb16fde49868d0d70521acc1e47",
        "IPY_MODEL_6105b71eb34c48b498c7336d4760aaee",
        "IPY_MODEL_fe9154f304524ea496a0dcfd904ea113"
       ],
       "layout": "IPY_MODEL_9119b63dc89c47de87d8c60b82efcc49",
       "tabbable": null,
       "tooltip": null
      }
     },
     "6105b71eb34c48b498c7336d4760aaee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fc766fe95c364c06897d20d84b4a48fc",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d4f684a1efc7493e96c15b82cb6f8786",
       "tabbable": null,
       "tooltip": null,
       "value": 2.0
      }
     },
     "9119b63dc89c47de87d8c60b82efcc49": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cd86ccb16fde49868d0d70521acc1e47": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0f3c7228871447a9b38f12cefa447b17",
       "placeholder": "​",
       "style": "IPY_MODEL_26eb68657e7d4352aff1256dace691b0",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "d4f684a1efc7493e96c15b82cb6f8786": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "fc766fe95c364c06897d20d84b4a48fc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fe9154f304524ea496a0dcfd904ea113": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0428287b02164e28bdf38fcfad11f2df",
       "placeholder": "​",
       "style": "IPY_MODEL_172c0566f091427585ea993d10b3964c",
       "tabbable": null,
       "tooltip": null,
       "value": " 2/2 [01:06&lt;00:00, 32.00s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
